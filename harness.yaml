apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-serviceaccount
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-role
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - configmaps
  - pods
  - secrets
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - update
  - watch
- apiGroups:
  - extensions
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - ""
  resourceNames:
  - ingress-controller-leader-harness
  resources:
  - configmaps
  verbs:
  - get
  - update
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - create
  - get
  - update
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    release: timescaledb-single-chart
  name: timescaledb-single-chart
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  - delete
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - endpoints
  - endpoints/restricted
  verbs:
  - create
  - get
  - patch
  - update
  - list
  - watch
  - delete
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-manager-role
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-role-hsa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: harness-role
subjects:
- kind: ServiceAccount
  name: harness-serviceaccount
  namespace: harness
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    release: timescaledb-single-chart
  name: timescaledb-single-chart
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: timescaledb-single-chart
subjects:
- kind: ServiceAccount
  name: harness-default
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-manager-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: harness-manager-role
subjects:
- kind: ServiceAccount
  name: harness-default
  namespace: harness
---
apiVersion: v1
data:
  proxy.conf: |-
    server { root /www/data;proxy_http_version 1.1;
    }
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: delegate-proxy
---
apiVersion: v1
data:
  API_VERSION: release-gateway:182
  CACHE_TYPE: REDIS
  DEPLOY_MODE: KUBERNETES_ONPREM
  LOG_SVC_GLOBAL_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  MANAGER_PUBLIC_URL: http://harness-url
  MANAGER_URL: http://harness-url
  MEMORY: "2048"
  REDIS_PORT: "26379"
  REDIS_SENTINELS: redis-sentinel-harness-announce-0.harness,redis-sentinel-harness-announce-1.harness,redis-sentinel-harness-announce-2.harness
  SENTINEL_MASTER_NAME: harness-redis
  TI_SVC_GLOBAL_TOKEN: 78d16b66-4b4c-11eb-8377-acde48001122
  TOKEN_CACHE_TTL: "300"
  USE_SENTINEL: "true"
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-gateway
---
apiVersion: v1
data:
  proxy-body-size: 1024m
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-ingress-controller
---
apiVersion: v1
data:
  "7909": harness/gitops-service:7909
  "9000": harness/log-service-minio:9000
  "9879": harness/harness-manager:9879
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-ingress-controller-tcp-services
---
apiVersion: v1
data:
  ACCESS_CONTROL_BASE_URL: http://harness-url/authz/api/
  ACCESS_CONTROL_ENABLED: "true"
  ALLOWED_ORIGINS: http://harness-url
  API_URL: http://harness-url
  ATMOSPHERE_BACKEND: REDIS
  BACKGROUND_SCHEDULER_CLUSTERED: "true"
  CACHE_BACKEND: REDIS
  CAPSULE_JAR: rest-capsule.jar
  DELEGATE_DOCKER_IMAGE: harness/delegate:latest
  DELEGATE_GRPC_AUTHORITY: default-authority.harness.io
  DELEGATE_GRPC_TARGET: harness-url:9879
  DELEGATE_METADATA_URL: http://harness-url/storage/wingsdelegates/delegateprod.txt
  DELEGATE_SERVICE_AUTHORITY: default-authority.harness.io
  DELEGATE_SERVICE_TARGET: localhost:9879
  DEPLOY_MODE: KUBERNETES_ONPREM
  DISABLE_NEW_RELIC: "true"
  DISTRIBUTED_LOCK_IMPLEMENTATION: REDIS
  ENABLE_G1GC: "true"
  EVENTS_FRAMEWORK_AVAILABLE_IN_ONPREM: "true"
  EVENTS_FRAMEWORK_REDIS_SENTINELS: redis://redis-sentinel-harness-announce-0.harness:26379,redis://redis-sentinel-harness-announce-1.harness:26379,redis://redis-sentinel-harness-announce-2.harness:26379
  EVENTS_FRAMEWORK_REDIS_URL: redis://localhost:6379
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: harness-redis
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EXTERNAL_GRAPHQL_RATE_LIMIT: "500"
  FEATURES: LDAP_SSO_PROVIDER,ASYNC_ARTIFACT_COLLECTION,JIRA_INTEGRATION,AUDIT_TRAIL_UI,GDS_TIME_SERIES_SAVE_PER_MINUTE,STACKDRIVER_SERVICEGUARD,BATCH_SECRET_DECRYPTION,TIME_SERIES_SERVICEGUARD_V2,TIME_SERIES_WORKFLOW_V2,CUSTOM_DASHBOARD,GRAPHQL,CV_FEEDBACKS,LOGS_V2_247,UPGRADE_JRE,CDNG_ENABLED,NEXT_GEN_ENABLED,LOG_STREAMING_INTEGRATION,CING_ENABLED,NG_HARNESS_APPROVAL,GIT_SYNC_NG,NG_SHOW_DELEGATE,NG_CG_TASK_ASSIGNMENT_ISOLATION,CI_OVERVIEW_PAGE,AZURE_CLOUD_PROVIDER_VALIDATION_ON_DELEGATE
  HAZELCAST_NAMESPACE: harness
  HAZELCAST_SERVICE: harness-manager
  HZ_CLUSTER_NAME: harness-manager
  LOG_STREAMING_SERVICE_BASEURL: http://harness-url/log-service/
  LOG_STREAMING_SERVICE_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  LOGGING_LEVEL: INFO
  MEMORY: "2048"
  NG_MANAGER_BASE_URL: http://harness-url/ng/api/
  REDIS_MASTER_NAME: harness-redis
  REDIS_SENTINEL: "true"
  REDIS_SENTINELS: redis://redis-sentinel-harness-announce-0.harness:26379,redis://redis-sentinel-harness-announce-1.harness:26379,redis://redis-sentinel-harness-announce-2.harness:26379
  REDIS_URL: redis://localhost:6379
  SERVER_PORT: "9090"
  SERVICE_ACC: /opt/harness/svc/service_acc.json
  UI_SERVER_URL: http://harness-url
  VERSION: 1.0.74410
  WATCHER_METADATA_URL: http://harness-url/storage/wingswatchers/watcherprod.txt
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-manager-config
---
apiVersion: v1
data:
  https_port: "10800"
  learning_env: on_prem
  server_url: http://verification-svc:7070
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: learning-engine
---
apiVersion: v1
data:
  on-start.sh: |
    #!/usr/bin/env bash

    # Copyright 2018 The Kubernetes Authors. All rights reserved.
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    set -e pipefail

    port=27017
    replica_set="$REPLICA_SET"
    script_name=${0##*/}
    SECONDS=0
    timeout="${TIMEOUT:-900}"

    if [[ "$AUTH" == "true" ]]; then
        admin_user="$ADMIN_USER"
        admin_password="$ADMIN_PASSWORD"
        admin_creds=(-u "$admin_user" -p "$admin_password")
        if [[ "$METRICS" == "true" ]]; then
            metrics_user="$METRICS_USER"
            metrics_password="$METRICS_PASSWORD"
        fi
        auth_args=("--auth" "--keyFile=/data/configdb/key.txt")
    fi

    log() {
        local msg="$1"
        local timestamp
        timestamp=$(date --iso-8601=ns)
        echo "[$timestamp] [$script_name] $msg" 2>&1 | tee -a /work-dir/log.txt 1>&2
    }

    retry_until() {
        local host="${1}"
        local command="${2}"
        local expected="${3}"
        local creds=("${admin_creds[@]}")

        # Don't need credentials for admin user creation and pings that run on localhost
        if [[ "${host}" =~ ^localhost ]]; then
            creds=()
        fi

        until [[ $(mongo admin --host "${host}" "${creds[@]}" "${ssl_args[@]}" --quiet --eval "${command}" | tail -n1) == "${expected}" ]]; do
            sleep 1

            if (! ps "${pid}" &>/dev/null); then
                log "mongod shutdown unexpectedly"
                exit 1
            fi
            if [[ "${SECONDS}" -ge "${timeout}" ]]; then
                log "Timed out after ${timeout}s attempting to bootstrap mongod"
                exit 1
            fi

            log "Retrying ${command} on ${host}"
        done
    }

    shutdown_mongo() {
        local host="${1:-localhost}"
        local args='force: true'
        log "Shutting down MongoDB ($args)..."
        if (! mongo admin --host "${host}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "db.shutdownServer({$args})"); then
          log "db.shutdownServer() failed, sending the terminate signal"
          kill -TERM "${pid}"
        fi
    }

    ca_pem=/data/ssl/ca.pem
    if [ -f "$ca_pem"  ]; then
      client_pem=/data/ssl/client.pem
      log "Changing ssl_args..."
      ssl_args=(--tls --tlsCAFile "$ca_pem" --tlsCertificateKeyFile "$client_pem")
    fi

    my_hostname=$(hostname)
    log "Bootstrapping MongoDB replica set member: $my_hostname"

    log "Reading standard input..."
    while read -ra line; do
        if [[ "${line}" == *"${my_hostname}"* ]]; then
            service_name="$line"
        fi
        peers=("${peers[@]}" "$line")
    done


    if [[ "${SKIP_INIT}" == "true" ]]; then
        log "Skipping initialization"
        exit 0
    fi

    log "Peers: ${peers[*]}"
    log "Starting a MongoDB replica"
    mongod --config /data/configdb/mongod.conf --dbpath=/data/db --replSet="$replica_set" --port="${port}" "${auth_args[@]}" --bind_ip=0.0.0.0 2>&1 | tee -a /work-dir/log.txt 1>&2 &
    pid=$!
    trap shutdown_mongo EXIT

    log "Waiting for MongoDB to be ready..."
    retry_until "localhost" "db.adminCommand('ping').ok" "1"
    log "Initialized."

    # try to find a master
    for peer in "${peers[@]}"; do
        log "Checking if ${peer} is primary"
        # Check rs.status() first since it could be in primary catch up mode which db.isMaster() doesn't show
        if [[ $(mongo admin --host "${peer}" "${admin_creds[@]}" "${ssl_args[@]}" --quiet --eval "rs.status().myState" | tail -n1) == "1" ]]; then
            retry_until "${peer}" "db.isMaster().ismaster" "true"
            log "Found primary: ${peer}"
            primary="${peer}"
            break
        fi
    done

    if [[ "${primary}" = "${service_name}" ]]; then
        log "This replica is already PRIMARY"
    elif [[ -n "${primary}" ]]; then
        if [[ $(mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --quiet --eval "rs.conf().members.findIndex(m => m.host == '${service_name}:${port}')" | tail -n1) == "-1" ]]; then
          log "Adding myself (${service_name}) to replica set..."
          if (mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "rs.add('${service_name}')" | grep 'Quorum check failed'); then
              log 'Quorum check failed, unable to join replicaset. Exiting prematurely.'
              exit 1
          fi
        fi

        sleep 3
        log 'Waiting for replica to reach SECONDARY state...'
        retry_until "${service_name}" "rs.status().myState" "2"
        log '✓ Replica reached SECONDARY state.'

    elif (mongo "${ssl_args[@]}" --eval "rs.status()" | grep "no replset config has been received"); then
        log "Initiating a new replica set with myself ($service_name)..."
        mongo "${ssl_args[@]}" --eval "rs.initiate({'_id': '$replica_set', 'members': [{'_id': 0, 'host': '$service_name'}]})"

        sleep 3
        log 'Waiting for replica to reach PRIMARY state...'
        retry_until "localhost" "db.isMaster().ismaster" "true"
        primary="${service_name}"
        log '✓ Replica reached PRIMARY state.'

        if [[ "${AUTH}" == "true" ]]; then
            log "Creating admin user..."
            mongo admin "${ssl_args[@]}" --eval "db.createUser({user: '${admin_user}', pwd: '${admin_password}', roles: [{role: 'root', db: 'admin'}]})"
        fi
    fi

    # User creation
    if [[ -n "${primary}" && "$AUTH" == "true" && "$METRICS" == "true" ]]; then
        metric_user_count=$(mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "db.system.users.find({user: '${metrics_user}'}).count()" --quiet | tail -n1)
        if [[ "${metric_user_count}" == "0" ]]; then
            log "Creating clusterMonitor user..."
            mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "db.createUser({user: '${metrics_user}', pwd: '${metrics_password}', roles: [{role: 'clusterMonitor', db: 'admin'}, {role: 'read', db: 'local'}]})"
        fi
    fi

    log "MongoDB bootstrap complete"
    exit 0
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: mongodb-replicaset
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: mongodb-replicaset-chart-init
---
apiVersion: v1
data:
  mongod.conf: |
    {}
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
  labels:
    app: mongodb-replicaset
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: mongodb-replicaset-chart-mongodb
---
apiVersion: v1
data:
  API_URL: /gateway
  DEPLOYMENT_TYPE: ON_PREM
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: ng-auth-ui
---
apiVersion: v1
data:
  haproxy_init.sh: |
    HAPROXY_CONF=/data/haproxy.cfg
    cp /readonly/haproxy.cfg "$HAPROXY_CONF"
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-0 && break
      echo "Waiting for service redis-sentinel-harness-announce-0 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP0=$(getent hosts "redis-sentinel-harness-announce-0" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP0" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-0"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE0/$ANNOUNCE_IP0/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-1 && break
      echo "Waiting for service redis-sentinel-harness-announce-1 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP1=$(getent hosts "redis-sentinel-harness-announce-1" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP1" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-1"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE1/$ANNOUNCE_IP1/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-2 && break
      echo "Waiting for service redis-sentinel-harness-announce-2 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP2=$(getent hosts "redis-sentinel-harness-announce-2" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP2" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-2"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE2/$ANNOUNCE_IP2/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
  init.sh: |
    HOSTNAME="$(hostname)"
    INDEX="${HOSTNAME##*-}"
    MASTER="$(redis-cli -h redis-sentinel-harness -p 26379 sentinel get-master-addr-by-name harness-redis | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
    MASTER_GROUP="harness-redis"
    QUORUM="2"
    REDIS_CONF=/data/conf/redis.conf
    REDIS_PORT=6379
    SENTINEL_CONF=/data/conf/sentinel.conf
    SENTINEL_PORT=26379
    SERVICE=redis-sentinel-harness
    set -eu

    sentinel_update() {
        echo "Updating sentinel config with master $MASTER"
        eval MY_SENTINEL_ID="\${SENTINEL_ID_$INDEX}"
        sed -i "1s/^/sentinel myid $MY_SENTINEL_ID\\n/" "$SENTINEL_CONF"
        sed -i "2s/^/sentinel monitor $MASTER_GROUP $1 $REDIS_PORT $QUORUM \\n/" "$SENTINEL_CONF"
        echo "sentinel announce-ip $ANNOUNCE_IP" >> $SENTINEL_CONF
        echo "sentinel announce-port $SENTINEL_PORT" >> $SENTINEL_CONF
    }

    redis_update() {
        echo "Updating redis config"
        echo "slaveof $1 $REDIS_PORT" >> "$REDIS_CONF"
        echo "slave-announce-ip $ANNOUNCE_IP" >> $REDIS_CONF
        echo "slave-announce-port $REDIS_PORT" >> $REDIS_CONF
    }

    copy_config() {
        cp /readonly-config/redis.conf "$REDIS_CONF"
        cp /readonly-config/sentinel.conf "$SENTINEL_CONF"
    }

    setup_defaults() {
        echo "Setting up defaults"
        if [ "$INDEX" = "0" ]; then
            echo "Setting this pod as the default master"
            redis_update "$ANNOUNCE_IP"
            sentinel_update "$ANNOUNCE_IP"
            sed -i "s/^.*slaveof.*//" "$REDIS_CONF"
        else
            DEFAULT_MASTER="$(getent hosts "$SERVICE-announce-0" | awk '{ print $1 }')"
            if [ -z "$DEFAULT_MASTER" ]; then
                echo "Unable to resolve host"
                exit 1
            fi
            echo "Setting default slave config.."
            redis_update "$DEFAULT_MASTER"
            sentinel_update "$DEFAULT_MASTER"
        fi
    }

    find_master() {
        echo "Attempting to find master"
        if [ "$(redis-cli -h "$MASTER" ping)" != "PONG" ]; then
           echo "Can't ping master, attempting to force failover"
           if redis-cli -h "$SERVICE" -p "$SENTINEL_PORT" sentinel failover "$MASTER_GROUP" | grep -q 'NOGOODSLAVE' ; then
               setup_defaults
               return 0
           fi
           sleep 10
           MASTER="$(redis-cli -h $SERVICE -p $SENTINEL_PORT sentinel get-master-addr-by-name $MASTER_GROUP | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
           if [ "$MASTER" ]; then
               sentinel_update "$MASTER"
               redis_update "$MASTER"
           else
              echo "Could not failover, exiting..."
              exit 1
           fi
        else
            echo "Found reachable master, updating config"
            sentinel_update "$MASTER"
            redis_update "$MASTER"
        fi
    }

    mkdir -p /data/conf/

    echo "Initializing config.."
    copy_config

    ANNOUNCE_IP=$(getent hosts "$SERVICE-announce-$INDEX" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP" ]; then
        "Could not resolve the announce ip for this pod"
        exit 1
    elif [ "$MASTER" ]; then
        find_master
    else
        setup_defaults
    fi

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/replace-default-auth/${ESCAPED_AUTH}/" "$REDIS_CONF" "$SENTINEL_CONF"
    fi

    echo "Ready..."
  redis.conf: |
    dir "/data"
    port 6379
    active-defrag-cycle-max 25
    active-defrag-ignore-bytes 1mb
    activedefrag yes
    maxmemory 0
    maxmemory-policy volatile-lru
    min-replicas-max-lag 10
    min-replicas-to-write 1
    rdbchecksum yes
    rdbcompression yes
    repl-diskless-sync yes
    save 60 1
    maxclients 30000
    timeout 10
  sentinel.conf: |
    dir "/data"
        sentinel down-after-milliseconds harness-redis 10000
        sentinel failover-timeout harness-redis 180000
        maxclients 30000
        sentinel parallel-syncs harness-redis 5
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: redis-sentinel-harness
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: redis-sentinel-harness-configmap
---
apiVersion: v1
data:
  on_start: |
    #!/bin/bash

    # This script should only run on the master instance, Patroni
    # passes on the role in the second parameter
    echo "Running timescaledb-init"
    [ "$2" != "master" ] && exit 0

    echo "SELECT 'CREATE DATABASE harness' WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'harness')\gexec" | psql

    echo "SELECT 'CREATE DATABASE harnessti' WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'harnessti')\gexec" | psql
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: timescaledb-init
---
apiVersion: v1
data:
  patroni.yaml: |
    bootstrap:
      dcs:
        loop_wait: 10
        maximum_lag_on_failover: 33554432
        postgresql:
          parameters:
            archive_command: /etc/timescaledb/scripts/pgbackrest_archive.sh %p
            archive_mode: "on"
            archive_timeout: 1800s
            autovacuum_analyze_scale_factor: 0.02
            autovacuum_max_workers: 10
            autovacuum_vacuum_scale_factor: 0.05
            hot_standby: "on"
            log_autovacuum_min_duration: 0
            log_checkpoints: "on"
            log_connections: "on"
            log_disconnections: "on"
            log_line_prefix: '%t [%p]: [%c-%l] %u@%d,app=%a [%e] '
            log_lock_waits: "on"
            log_min_duration_statement: 1s
            log_statement: ddl
            max_connections: 100
            max_prepared_transactions: 150
            shared_preload_libraries: timescaledb,pg_stat_statements
            ssl: "on"
            ssl_cert_file: /etc/certificate/tls.crt
            ssl_key_file: /etc/certificate/tls.key
            tcp_keepalives_idle: 900
            tcp_keepalives_interval: 100
            temp_file_limit: 1GB
            timescaledb.passfile: ../.pgpass
            unix_socket_directories: /var/run/postgresql
            unix_socket_permissions: "0750"
            wal_level: hot_standby
            wal_log_hints: "on"
          use_pg_rewind: true
          use_slots: true
        retry_timeout: 10
        ttl: 30
      method: restore_or_initdb
      post_init: /etc/timescaledb/scripts/post_init.sh
      restore_or_initdb:
        command: |
          /etc/timescaledb/scripts/restore_or_initdb.sh --encoding=UTF8 --locale=C.UTF-8
        keep_existing_recovery_conf: true
    kubernetes:
      ports:
      - name: postgresql
        port: 5432
        targetPort: 5432
      role_label: role
      scope_label: cluster-name
      use_endpoints: true
    log:
      level: WARNING
    postgresql:
      authentication:
        replication:
          username: standby
        superuser:
          username: postgres
      basebackup:
      - waldir: /var/lib/postgresql/wal/pg_wal
      callbacks:
        on_reload: /etc/timescaledb/scripts/patroni_callback.sh
        on_restart: /etc/timescaledb/scripts/patroni_callback.sh
        on_role_change: /etc/timescaledb/scripts/patroni_callback.sh
        on_start: /etc/timescaledb/scripts/patroni_callback.sh
        on_stop: /etc/timescaledb/scripts/patroni_callback.sh
      create_replica_methods:
      - pgbackrest
      - basebackup
      listen: 0.0.0.0:5432
      pg_hba:
      - hostnossl all,replication all                all                reject
      - local     all             all                                   peer
      - hostssl   all             all                127.0.0.1/32       md5
      - hostssl   all             all                ::1/128            md5
      - hostssl   replication     standby            all                md5
      - hostssl   all             all                all                md5
      pgbackrest:
        command: /etc/timescaledb/scripts/pgbackrest_restore.sh
        keep_data: true
        no_master: 1
        no_params: true
      recovery_conf:
        restore_command: /etc/timescaledb/scripts/pgbackrest_archive_get.sh %f "%p"
      use_unix_socket: true
    restapi:
      listen: 0.0.0.0:8008
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    cluster-name: timescaledb-single-chart
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    release: timescaledb-single-chart
  name: timescaledb-single-chart-patroni
---
apiVersion: v1
data:
  lifecycle_preStop.psql: |
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''

    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '/var/run/postgresql', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset

    \timing on
    \set ECHO queries

    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif

    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;

    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
  patroni_callback.sh: |
    #!/bin/bash
    set -e

    source "${HOME}/.pod_environment"

    for suffix in "$1" all
    do
      CALLBACK="/etc/timescaledb/callbacks/${suffix}"
      if [ -f "${CALLBACK}" ]
      then
        "${CALLBACK}" $@
      fi
    done
  pgbackrest_archive.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 0

    source "${HOME}/.pgbackrest_environment"
    exec pgbackrest --stanza=poddb archive-push $1
  pgbackrest_archive_get.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 1

    source "${HOME}/.pgbackrest_environment"
    exec pgbackrest --stanza=poddb archive-get ${1} "${2}"
  pgbackrest_bootstrap.sh: |
    #!/bin/bash
    set -e

    function log {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }

    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done

    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" == "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
        }
    fi

    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
  pgbackrest_restore.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 1

    source "${HOME}/.pod_environment"

    PGDATA="/var/lib/postgresql/data"
    WALDIR="/var/lib/postgresql/wal/pg_wal"

    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"

    pgbackrest --stanza=poddb --force --delta --log-level-console=detail restore
  post_init.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0

    source "${HOME}/.pod_environment"

    function log {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }

    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
      \connect template1
      -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;

      \connect postgres
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;
    __SQL__

    TABLESPACES=""
    for tablespace in $TABLESPACES
    do
      log "Creating tablespace ${tablespace}"
      tablespacedir="/var/lib/postgresql/tablespaces/${tablespace}/data"
      psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done

    if [ "${PGBACKREST_BACKUP_ENABLED}" == "1" ]; then
      log "Waiting for pgBackRest API to become responsive"
      while sleep 1; do
          if [ $SECONDS -gt 10 ]; then
              log "pgBackRest API did not respond within $SECONDS seconds, will not trigger a backup"
              exit 0
          fi
          timeout 1 bash -c "echo > /dev/tcp/localhost/8081" 2>/dev/null && break
      done

      log "Triggering pgBackRest backup"
      curl -i -X POST http://localhost:8081/backups
    fi

    # We always exit 0 this script, otherwise the database initialization fails.
    exit 0
  restore_or_initdb.sh: |
    #!/bin/bash

    source "${HOME}/.pod_environment"

    function log {
      echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }

    PGDATA="/var/lib/postgresql/data"
    WALDIR="/var/lib/postgresql/wal/pg_wal"

    # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
    # initdb has no business with these parameters
    initdb_args=""
    for value in "$@"
    do
      [[ $value == --scope* ]] || [[ $value == --datadir* ]] || initdb_args="${initdb_args} $value"
    done

    log "Invoking initdb"
    initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    echo "include_if_exists = '/var/run/postgresql/timescaledb.conf'" >> "${PGDATA}/postgresql.conf"
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    cluster-name: timescaledb-single-chart
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    release: timescaledb-single-chart
  name: timescaledb-single-chart-scripts
---
apiVersion: v1
data:
  API_URL: http://harness-url/gateway
  HARNESS_ENABLE_DRIFT_PLACEHOLDER: "false"
  HARNESS_ENABLE_NG_AUTH_UI_PLACEHOLDER: "true"
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: ui
---
apiVersion: v1
data:
  DEPLOY_MODE: KUBERNETES_ONPREM
  LOGGING_LEVEL: INFO
  MANAGER_URL: http://harness-manager:9090/api/
  MEMORY: "2048"
  VERIFICATION_PORT: "7070"
  VERSION: 1.0.72506
kind: ConfigMap
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: verification-svc
---
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJwcm94eS5yZXBsaWNhdGVkLmNvbSI6eyJhdXRoIjoiTVhoQ1dWTnBTM05xUTBwbGJVZFJZMUZNWVZSRFVHbDRiVWt4T2pGNFFsbFRhVXR6YWtOS1pXMUhVV05SVEdGVVExQnBlRzFKTVE9PSJ9LCJyZWdpc3RyeS5yZXBsaWNhdGVkLmNvbSI6eyJhdXRoIjoiTVhoQ1dWTnBTM05xUTBwbGJVZFJZMUZNWVZSRFVHbDRiVWt4T2pGNFFsbFRhVXR6YWtOS1pXMUhVV05SVEdGVVExQnBlRzFKTVE9PSJ9fX0=
kind: Secret
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-9999"
    kots.io/app-slug: harness
  creationTimestamp: null
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-registry
  namespace: harness
type: kubernetes.io/dockerconfigjson
---
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJwcm94eS5yZXBsaWNhdGVkLmNvbSI6eyJhdXRoIjoiTVhoQ1dWTnBTM05xUTBwbGJVZFJZMUZNWVZSRFVHbDRiVWt4T2pGNFFsbFRhVXR6YWtOS1pXMUhVV05SVEdGVVExQnBlRzFKTVE9PSJ9LCJyZWdpc3RyeS5yZXBsaWNhdGVkLmNvbSI6eyJhdXRoIjoiTVhoQ1dWTnBTM05xUTBwbGJVZFJZMUZNWVZSRFVHbDRiVWt4T2pGNFFsbFRhVXR6YWtOS1pXMUhVV05SVEdGVVExQnBlRzFKTVE9PSJ9fX0=
kind: Secret
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-9999"
    kots.io/app-slug: harness
  creationTimestamp: null
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: kotsadm-replicated-registry
  namespace: harness
type: kubernetes.io/dockerconfigjson
---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-gateway
stringData:
  MONGO_DB_URL: mongodb://admin:CA8FMywpbM@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness.svc:27017/gateway?replicaSet=rs0&authSource=admin
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-manager-config
stringData:
  LICENSE_INFO: su3x47cXBqhiaIqaFkPa7o7nMD4tgWGJlwn9EovOHi0xJjasm5SyQCsmYR10pgSsbFLW1jOSsg1ZY/kI5d886w==
  MONGO_URI: mongodb://admin:CA8FMywpbM@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness.svc.cluster.local,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness.svc.cluster.local,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness.svc.cluster.local:27017/harness?replicaSet=rs0&authSource=admin
  TIMESCALEDB_PASSWORD: kXsMSRiREYXiMciUwMdj
  TIMESCALEDB_URI: jdbc:postgresql://timescaledb-single-chart.harness:5432/harness
  TIMESCALEDB_USERNAME: postgres
  VERIFICATION_SERVICE_SECRET: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: learning-engine
stringData:
  service_secret: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8
type: Opaque
---
apiVersion: v1
data:
  password: Q0E4Rk15d3BiTQ==
  user: YWRtaW4=
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: mongodb-replicaset
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: mongodb-replicaset-chart-admin
type: Opaque
---
apiVersion: v1
data:
  key.txt: b0x0dVV4dmN6VmJ3a2ViRnRoT0E=
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: mongodb-replicaset
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: mongodb-replicaset-chart-keyfile
type: Opaque
---
apiVersion: v1
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lSQU1qSkV5YjVzcWl3N0paWUk0eGpLcFF3RFFZSktvWklodmNOQVFFTEJRQXcKSXpFaE1COEdBMVVFQXhNWWRHbHRaWE5qWVd4bFpHSXRjMmx1WjJ4bExXTm9ZWEowTUI0WERUSXhNRFF3T1RBMQpOVFUwT0ZvWERUSTJNRFF3T1RBMU5UVTBPRm93SXpFaE1COEdBMVVFQXhNWWRHbHRaWE5qWVd4bFpHSXRjMmx1CloyeGxMV05vWVhKME1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBdVZvM0tuMS8KTzV3aUxWdkF1MWFOeWVIcXRjQU80RXJGOUkxOUh3NkZFR1MrbzdNQmZrU2U5UGFUQnJQeGV3cTNvM1kxaVdYUApGMDJiQ3RSUEMzRmxGQUZPMTRmYmJra1YxNVpYdmdxUlpuaU1Zb0Q0SmRwWDJPSm81Vm1iaVFnOVJBSkJJbVR5CkdGRnowb0lMVTI0Z05jRUZzZEtPSUFhR0NCRG9iRFpDM1MwbWxPQ3JZOE9GU2VpMnVTVGhvWkE5RFpCbG1WQkcKMkMyeDFQYkNGWmtYT2U3MlRtQ1IzcW9nM0JvOVRhSngxMUtjWE1sNzRrbUFmWlZob2Y2dE14cll1dnJTWFdaVQpKY25XdjlaZmxReWZXZ0QvOUljcWpOWXFmdkpkaTc5Z05aS05YME1lSnZYOG5OdjJzR29QcFloOGNpWmxRR29hCnk4eWw2TVc2WXVDM2h3SURBUUFCbzBJd1FEQU9CZ05WSFE4QkFmOEVCQU1DQXFRd0hRWURWUjBsQkJZd0ZBWUkKS3dZQkJRVUhBd0VHQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFGZ285MWtMd2xDeExqYkhqdXdTNnFOSWJxOVRicWdDb2R4WW9Jb25OS0huRDdwYVZMM2JuMjdFCmUxQmZLTjVJcGd4ODEyeTVNbW5sNG9wS2NuMmpSWW0zNDFjeHd3L1gweFVqclNTMnF3VTZpNXdjRjBDS0M1Q3gKM3FHZU9aZ3FwekovcHJ1Y2NnNnVZVlBHSFYwUkJ1ZkQzYXZ6RlFsM3FjVUJESE9PUUVwUW5wR25uSU1qbHM0NQpzOURKdWkzYnBCYWRGaVFqeXdnNXA4UnlZSUMvcTVvMzRVb1JXNGI0RnM3Z1BNWnNtRmFTaU9xS1dmRGxVOU92Ck51MUFDNFNtRkgyVXYrZVc0UEhWUklMR0xoeDI3M0JUcVJnMzFIbE8rVmpMNzBrd2tWVDkrS04zclFVUlR5cGgKdC9hYWdDQ2tCWDVPVmxBTnhpRk9mTmM3ekJpb05QYz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  tls.key: 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
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    cluster-name: timescaledb-single-chart
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    release: timescaledb-single-chart
  name: timescaledb-single-chart-certificate
type: kubernetes.io/tls
---
apiVersion: v1
data:
  admin: amFGenFoTFBaV0JPS1VScmN5WXY=
  postgres: a1hzTVNSaVJFWVhpTWNpVXdNZGo=
  standby: YVVZaGNVUXBMVlRrR2J1ZWZKVW0=
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    release: timescaledb-single-chart
  name: timescaledb-single-chart-passwords
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: verification-svc
stringData:
  MONGO_URI: mongodb://admin:CA8FMywpbM@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness.svc.cluster.local,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness.svc.cluster.local,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness.svc.cluster.local:27017/harness?replicaSet=rs0&authSource=admin
  VERIFICATION_SERVICE_SECRET: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: default-backend
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: default-backend
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: delegate-proxy
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: delegate-proxy
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-gateway
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: harness-gateway
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
    kots.io/placeholder: "true"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-ingress-controller
spec:
  externalTrafficPolicy: Cluster
  loadBalancerIP: 35.202.24.14
  ports:
  - name: http
    nodePort: 32500
    port: 80
    protocol: TCP
    targetPort: http
  - name: https
    nodePort: 32505
    port: 443
    protocol: TCP
    targetPort: https
  - name: minio
    nodePort: 32507
    port: 9000
    protocol: TCP
    targetPort: 9000
  - name: gitops-grpc
    nodePort: 32511
    port: 7909
    protocol: TCP
    targetPort: 7909
  - name: grpc
    nodePort: 32510
    port: 9879
    protocol: TCP
    targetPort: 9879
  selector:
    app: harness-ingress-controller
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    hazelcast-cluster: harness-manager
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-manager
spec:
  ports:
  - name: http-manager
    port: 9090
    protocol: TCP
    targetPort: 9090
  - name: manager-grpc
    port: 9879
    protocol: TCP
    targetPort: 9879
  selector:
    app: harness-manager
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-ui
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: harness-ui
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  labels:
    app: mongodb-replicaset
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: mongodb-replicaset-chart
spec:
  clusterIP: None
  ports:
  - name: mongodb
    port: 27017
  publishNotReadyAddresses: true
  selector:
    app: mongodb-replicaset
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: ng-auth-ui
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: ng-auth-ui
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: redis-sentinel
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: redis-sentinel-harness
spec:
  clusterIP: None
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    app: redis-sentinel
    release: redis-ha
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  labels:
    app: redis-sentinel
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: redis-sentinel-harness-announce-0
spec:
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  publishNotReadyAddresses: true
  selector:
    app: redis-sentinel
    release: redis-ha
    statefulset.kubernetes.io/pod-name: redis-sentinel-harness-server-0
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  labels:
    app: redis-sentinel
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: redis-sentinel-harness-announce-1
spec:
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  publishNotReadyAddresses: true
  selector:
    app: redis-sentinel
    release: redis-ha
    statefulset.kubernetes.io/pod-name: redis-sentinel-harness-server-1
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  labels:
    app: redis-sentinel
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: redis-sentinel-harness-announce-2
spec:
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  publishNotReadyAddresses: true
  selector:
    app: redis-sentinel
    release: redis-ha
    statefulset.kubernetes.io/pod-name: redis-sentinel-harness-server-2
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "4000"
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    cluster-name: timescaledb-single-chart
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
    release: timescaledb-single-chart
    role: master
  name: timescaledb-single-chart
spec:
  ports:
  - name: postgresql
    port: 5432
    protocol: TCP
    targetPort: postgresql
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    cluster-name: timescaledb-single-chart
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
    release: timescaledb-single-chart
  name: timescaledb-single-chart-config
spec:
  clusterIP: None
  ports:
  - name: patroni
    port: 8008
    protocol: TCP
  selector:
    app: timescaledb-single-chart
    cluster-name: timescaledb-single-chart
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "4000"
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    cluster-name: timescaledb-single-chart
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
    release: timescaledb-single-chart
    role: replica
  name: timescaledb-single-chart-replica
spec:
  ports:
  - name: postgresql
    port: 5432
    protocol: TCP
  selector:
    app: timescaledb-single-chart
    cluster-name: timescaledb-single-chart
    role: replica
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kots.io/app-slug: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: verification-svc
spec:
  ports:
  - name: verification
    port: 7070
    protocol: TCP
    targetPort: 7070
  selector:
    app: verification-svc
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
    kots.io/placeholder: "true"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: default-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: default-backend
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: default-backend
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      containers:
      - image: k8s.gcr.io/defaultbackend-amd64:1.5
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        name: default-http-backend
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
          requests:
            cpu: 10m
            memory: 20Mi
        securityContext:
          runAsUser: 65534
      serviceAccountName: harness-serviceaccount
      terminationGracePeriodSeconds: 60
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: delegate-proxy
spec:
  selector:
    matchLabels:
      app: delegate-proxy
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: delegate-proxy
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      containers:
      - image: harness/delegate-proxy-signed:74410
        imagePullPolicy: IfNotPresent
        name: delegate-proxy
        resources:
          limits:
            cpu: 200m
            memory: 100Mi
          requests:
            cpu: 200m
            memory: 100Mi
        volumeMounts:
        - mountPath: /etc/nginx/conf.d
          name: harness-nginx-conf
      securityContext:
        runAsUser: 101
      serviceAccountName: harness-default
      volumes:
      - configMap:
          items:
          - key: proxy.conf
            path: proxy.conf
          name: delegate-proxy
        name: harness-nginx-conf
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    app: harness-gateway
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: harness-gateway
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: harness-gateway
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: harness-gateway
            topologyKey: kubernetes.io/hostname
      containers:
      - envFrom:
        - configMapRef:
            name: harness-gateway
        - secretRef:
            name: harness-gateway
        image: harness/gateway-signed:100089
        imagePullPolicy: Always
        livenessProbe:
          failureThreshold: 2
          httpGet:
            path: /actuator/health
            port: gateway-port
          initialDelaySeconds: 180
          periodSeconds: 20
        name: harness-gateway
        ports:
        - containerPort: 8080
          name: gateway-port
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /actuator/health
            port: gateway-port
          initialDelaySeconds: 120
          periodSeconds: 10
        resources:
          limits:
            cpu: "0.5"
            memory: 3072Mi
          requests:
            cpu: "0.5"
            memory: 3072Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/exclude: "false"
    kots.io/placeholder: "true"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-ingress-controller
spec:
  progressDeadlineSeconds: 300
  replicas: 1
  selector:
    matchLabels:
      app: harness-ingress-controller
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: harness-ingress-controller
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: harness-ingress-controller
            topologyKey: kubernetes.io/hostname
      containers:
      - args:
        - /nginx-ingress-controller
        - --ingress-class=harness
        - --default-backend-service=$(POD_NAMESPACE)/default-backend
        - --election-id=ingress-controller-leader
        - --watch-namespace=$(POD_NAMESPACE)
        - --update-status=true
        - --configmap=$(POD_NAMESPACE)/harness-ingress-controller
        - --http-port=8080
        - --https-port=8443
        - --tcp-services-configmap=$(POD_NAMESPACE)/harness-ingress-controller-tcp-services
        - --default-ssl-certificate=$(POD_NAMESPACE)/harness-cert
        - --publish-service=$(POD_NAMESPACE)/harness-ingress-controller
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        envFrom:
        - configMapRef:
            name: harness-ingress-controller
        image: us.gcr.io/k8s-artifacts-prod/ingress-nginx/controller:v0.47.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        name: nginx-ingress-controller
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        - containerPort: 8443
          name: https
          protocol: TCP
        - containerPort: 9000
          name: minio
          protocol: TCP
        - containerPort: 7909
          name: gitops-grpc
          protocol: TCP
        - containerPort: 9879
          name: grpc
          protocol: TCP
        resources:
          limits:
            cpu: "0.5"
            memory: 512Mi
          requests:
            cpu: "0.5"
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
      securityContext:
        runAsUser: 101
      serviceAccountName: harness-serviceaccount
      terminationGracePeriodSeconds: 60
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-manager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: harness-manager
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
        license-features-hash: 1855a3429258c196623423ca6b7aad7385b18aea62ef5ca6d66f05a3b2d95697
      creationTimestamp: null
      labels:
        app: harness-manager
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: harness-manager
            topologyKey: kubernetes.io/hostname
      containers:
      - envFrom:
        - configMapRef:
            name: harness-manager-config
        - secretRef:
            name: harness-manager-config
        image: harness/manager-signed:74410
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - touch
              - shutdown
        livenessProbe:
          failureThreshold: 20
          httpGet:
            path: /api/version
            port: 9090
            scheme: HTTP
          initialDelaySeconds: 180
          periodSeconds: 20
          successThreshold: 1
          timeoutSeconds: 1
        name: manager
        ports:
        - containerPort: 9879
          protocol: TCP
        readinessProbe:
          failureThreshold: 6
          httpGet:
            path: /api/health
            port: 9090
            scheme: HTTP
          initialDelaySeconds: 90
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "2"
            memory: 3000Mi
          requests:
            cpu: "2"
            memory: 3000Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
      initContainers:
      - args:
        - -c
        - "if [ -s /mongo/ca.pem ]; then \n  ssl_args=\"--tls --tlsCAFile /mongo/ca.pem
          --tlsCertificateKeyFile /mongo/client.pem\"; \nfi\nuntil mongo \"$(cat /config/MONGO_URI)\"
          $ssl_args --eval \"db.adminCommand('ping')\"; do echo waiting for mongodb;
          sleep 2; done\n"
        command:
        - sh
        image: harness/mongo:4.2.8
        name: check-for-mongo
        securityContext:
          runAsNonRoot: true
          runAsUser: 999
        volumeMounts:
        - mountPath: /config
          name: harness-manager-config
      - command:
        - sh
        - -c
        - until pg_isready -h timescaledb-single-chart; do echo waiting for timescaledb;
          sleep 2; done
        image: timescaledev/timescaledb-ha:pg11.11-ts2.1.0-p1
        name: check-for-timescaledb
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
      serviceAccountName: harness-default
      volumes:
      - name: harness-manager-config
        secret:
          items:
          - key: MONGO_URI
            path: MONGO_URI
          secretName: harness-manager-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: harness-ui
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: harness-ui
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
        license-features-hash: 1855a3429258c196623423ca6b7aad7385b18aea62ef5ca6d66f05a3b2d95697
      labels:
        app: harness-ui
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: harness-ui
            topologyKey: kubernetes.io/hostname
      containers:
      - envFrom:
        - configMapRef:
            name: ui
        image: proxy.replicated.com/proxy/harness/harness/ui-signed:73701
        imagePullPolicy: IfNotPresent
        name: ui
        ports:
        - containerPort: 8080
          protocol: TCP
        resources:
          limits:
            cpu: "0.5"
            memory: 512Mi
          requests:
            cpu: "0.5"
            memory: 512Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 101
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: harness-registry
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 15
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    app: learning-engine
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: learning-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      app: learning-engine
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: learning-engine
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: learning-engine
            topologyKey: kubernetes.io/hostname
      containers:
      - envFrom:
        - configMapRef:
            name: learning-engine
        - secretRef:
            name: learning-engine
        image: proxy.replicated.com/proxy/harness/harness/learning-engine-onprem-signed:66100
        imagePullPolicy: IfNotPresent
        name: learning-engine
        ports:
        - containerPort: 8108
          name: learning
          protocol: TCP
        resources:
          limits:
            cpu: "1"
            memory: 2048Mi
          requests:
            cpu: "1"
            memory: 2048Mi
        securityContext:
          runAsNonRoot: true
      imagePullSecrets:
      - name: harness-registry
      serviceAccountName: harness-default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    app: ng-auth-ui
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: ng-auth-ui
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: ng-auth-ui
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
        license-features-hash: 1855a3429258c196623423ca6b7aad7385b18aea62ef5ca6d66f05a3b2d95697
      labels:
        app: ng-auth-ui
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: ng-auth-ui
            topologyKey: kubernetes.io/hostname
      containers:
      - envFrom:
        - configMapRef:
            name: ng-auth-ui
        image: harness/ng-auth-ui-signed:0.33.2
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 2
          httpGet:
            path: /health
            port: ng-auth-ui-port
          initialDelaySeconds: 90
          periodSeconds: 20
        name: ng-auth-ui
        ports:
        - containerPort: 8080
          name: ng-auth-ui-port
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /health
            port: ng-auth-ui-port
          initialDelaySeconds: 15
          periodSeconds: 10
        resources:
          limits:
            cpu: "0.5"
            memory: 512Mi
          requests:
            cpu: "0.5"
            memory: 512Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 101
      serviceAccountName: harness-default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    app: verification-svc
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: verification-svc
spec:
  replicas: 1
  selector:
    matchLabels:
      app: verification-svc
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
        license-features-hash: 1855a3429258c196623423ca6b7aad7385b18aea62ef5ca6d66f05a3b2d95697
      labels:
        app: verification-svc
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: verification-svc
            topologyKey: kubernetes.io/hostname
      containers:
      - envFrom:
        - configMapRef:
            name: verification-svc
        - secretRef:
            name: verification-svc
        image: proxy.replicated.com/proxy/harness/harness/verification-service-signed:74410
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 2
          httpGet:
            path: /verification/health
            port: verification
            scheme: HTTP
          initialDelaySeconds: 300
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: verification
        ports:
        - containerPort: 7070
          name: verification
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /verification/health
            port: verification
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "1"
            memory: 3000Mi
          requests:
            cpu: "1"
            memory: 3000Mi
        securityContext:
          runAsNonRoot: true
      imagePullSecrets:
      - name: harness-registry
      initContainers:
      - args:
        - -c
        - "if [ -s /mongo/ca.pem ]; then \n  ssl_args=\"--tls --tlsCAFile /mongo/ca.pem
          --tlsCertificateKeyFile /mongo/client.pem\"; \nfi\nuntil mongo \"$(cat /config/MONGO_URI)\"
          $ssl_args --eval \"db.adminCommand('ping')\"; do echo waiting for mongodb;
          sleep 2; done\n"
        command:
        - sh
        image: harness/mongo:4.2.8
        name: check-for-mongo
        securityContext:
          runAsNonRoot: true
          runAsUser: 999
        volumeMounts:
        - mountPath: /config
          name: verification-svc
      restartPolicy: Always
      serviceAccountName: harness-default
      volumes:
      - name: verification-svc
        secret:
          items:
          - key: MONGO_URI
            path: MONGO_URI
          secretName: verification-svc
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    app: mongodb-replicaset
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
  name: mongodb-replicaset-chart
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mongodb-replicaset
  serviceName: mongodb-replicaset-chart
  template:
    metadata:
      annotations:
        backup.velero.io/backup-volumes: datadir
        checksum/config: fca3516170d327de50810cf50494e20bed3e41732799a291de1d1563b1a247cf
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: mongodb-replicaset
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: mongodb-replicaset
            topologyKey: kubernetes.io/hostname
      containers:
      - args:
        - --config=/data/configdb/mongod.conf
        - --dbpath=/data/db
        - --replSet=rs0
        - --port=27017
        - --bind_ip=0.0.0.0
        - --auth
        - --keyFile=/data/configdb/key.txt
        - --wiredTigerCacheSizeGB=3
        command:
        - mongod
        image: harness/mongo:4.2.8
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          initialDelaySeconds: 15
          periodSeconds: 10
          successThreshold: 1
          tcpSocket:
            port: 27017
          timeoutSeconds: 5
        name: mongodb-replicaset
        ports:
        - containerPort: 27017
          name: mongodb
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          tcpSocket:
            port: 27017
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "2"
            memory: 4096Mi
          requests:
            cpu: "2"
            memory: 4096Mi
        volumeMounts:
        - mountPath: /data/db
          name: datadir
        - mountPath: /data/configdb
          name: configdir
        - mountPath: /work-dir
          name: workdir
      initContainers:
      - args:
        - -c
        - |
          set -e
          set -x

          cp /configdb-readonly/mongod.conf /data/configdb/mongod.conf
          cp /keydir-readonly/key.txt /data/configdb/key.txt
          chmod 600 /data/configdb/key.txt
        command:
        - sh
        image: busybox:1.33.1
        imagePullPolicy: IfNotPresent
        name: copy-config
        resources: {}
        volumeMounts:
        - mountPath: /work-dir
          name: workdir
        - mountPath: /configdb-readonly
          name: config
        - mountPath: /data/configdb
          name: configdir
        - mountPath: /keydir-readonly
          name: keydir
      - args:
        - --work-dir=/work-dir
        image: harness/mongodb-install:0.8
        imagePullPolicy: IfNotPresent
        name: install
        resources: {}
        volumeMounts:
        - mountPath: /work-dir
          name: workdir
      - args:
        - -on-start=/init/on-start.sh
        - -service=mongodb-replicaset-chart
        command:
        - /work-dir/peer-finder
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: REPLICA_SET
          value: rs0
        - name: TIMEOUT
          value: "180"
        - name: SKIP_INIT
          value: "false"
        - name: AUTH
          value: "true"
        - name: ADMIN_USER
          valueFrom:
            secretKeyRef:
              key: user
              name: mongodb-replicaset-chart-admin
        - name: ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              key: password
              name: mongodb-replicaset-chart-admin
        image: harness/mongo:4.2.8
        imagePullPolicy: IfNotPresent
        name: bootstrap
        resources: {}
        volumeMounts:
        - mountPath: /work-dir
          name: workdir
        - mountPath: /init
          name: init
        - mountPath: /data/configdb
          name: configdir
        - mountPath: /data/db
          name: datadir
      securityContext:
        fsGroup: 999
        runAsNonRoot: true
        runAsUser: 999
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          name: mongodb-replicaset-chart-mongodb
        name: config
      - configMap:
          defaultMode: 493
          name: mongodb-replicaset-chart-init
        name: init
      - name: keydir
        secret:
          defaultMode: 256
          secretName: mongodb-replicaset-chart-keyfile
      - emptyDir: {}
        name: workdir
      - emptyDir: {}
        name: configdir
  volumeClaimTemplates:
  - metadata:
      annotations: null
      name: datadir
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 200Gi
      storageClassName: standard
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    app: redis-sentinel
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
    redis-sentinel-harness: replica
  name: redis-sentinel-harness-server
spec:
  podManagementPolicy: OrderedReady
  replicas: 3
  selector:
    matchLabels:
      app: redis-sentinel
      release: redis-ha
  serviceName: redis-sentinel-harness
  template:
    metadata:
      annotations:
        checksum/init-config: 0fb17318c62ec6e7f89897284e4d3edf7b1a0fc156692f8b15db8fd976df2e48
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: redis-sentinel
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
        redis-sentinel-harness: replica
        release: redis-ha
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: redis-sentinel
                  redis-sentinel-harness: replica
                  release: redis-ha
              topologyKey: failure-domain.beta.kubernetes.io/zone
            weight: 100
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: redis-sentinel
                release: redis-ha
            topologyKey: kubernetes.io/hostname
      containers:
      - args:
        - /data/conf/redis.conf
        command:
        - redis-server
        env: null
        image: harness/redis:6.2.5-alpine
        imagePullPolicy: IfNotPresent
        livenessProbe:
          initialDelaySeconds: 15
          tcpSocket:
            port: 6379
        name: redis
        ports:
        - containerPort: 6379
          name: redis
        resources:
          limits:
            cpu: "1"
            memory: 2048Mi
          requests:
            cpu: "1"
            memory: 2048Mi
        volumeMounts:
        - mountPath: /data
          name: data
      - args:
        - /data/conf/sentinel.conf
        command:
        - redis-sentinel
        image: harness/redis:6.2.5-alpine
        imagePullPolicy: IfNotPresent
        livenessProbe:
          initialDelaySeconds: 15
          tcpSocket:
            port: 26379
        name: sentinel
        ports:
        - containerPort: 26379
          name: sentinel
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - mountPath: /data
          name: data
      initContainers:
      - args:
        - /readonly-config/init.sh
        command:
        - sh
        env:
        - name: SENTINEL_ID_0
          value: ed89975e57ea5a6848fe664901b11b5e6b22b537
        - name: SENTINEL_ID_1
          value: 4abd57ef009b0a1595767af80ef815e3438ae7e9
        - name: SENTINEL_ID_2
          value: e21a3c2cf7abdfc6e8d921901addb1bf86fe32a0
        image: harness/redis:6.2.5-alpine
        imagePullPolicy: IfNotPresent
        name: config-init
        resources: {}
        volumeMounts:
        - mountPath: /readonly-config
          name: config
          readOnly: true
        - mountPath: /data
          name: data
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      serviceAccountName: harness-default
      volumes:
      - configMap:
          name: redis-sentinel-harness-configmap
        name: config
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      annotations: null
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName: standard
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    cluster-name: timescaledb-single-chart
    heritage: Tiller
    kots.io/app-slug: harness
    kots.io/backup: velero
    kots.io/placeholder: "true"
    release: timescaledb-single-chart
  name: timescaledb-single-chart
spec:
  replicas: 2
  selector:
    matchLabels:
      app: timescaledb-single-chart
      release: timescaledb-single-chart
  serviceName: timescaledb-single-chart
  template:
    metadata:
      annotations:
        backup.velero.io/backup-volumes: storage-volume
        kots.io/app-slug: harness
        kots.io/placeholder: "true"
      labels:
        app: timescaledb-single-chart
        cluster-name: timescaledb-single-chart
        kots.io/app-slug: harness
        kots.io/backup: velero
        kots.io/placeholder: "true"
        release: timescaledb-single-chart
      name: timescaledb-single-chart
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: timescaledb-single-chart
                cluster-name: timescaledb-single-chart
                release: timescaledb-single-chart
            topologyKey: kubernetes.io/hostname
      containers:
      - command:
        - /bin/bash
        - -c
        - |2

          install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/data" "/var/lib/postgresql/wal/pg_wal" || exit 1
          TABLESPACES=""
          for tablespace in ; do
            install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/tablespaces/${tablespace}/data"
          done

          # Environment variables can be read by regular users of PostgreSQL. Especially in a Kubernetes
          # context it is likely that some secrets are part of those variables.
          # To ensure we expose as little as possible to the underlying PostgreSQL instance, we have a list
          # of allowed environment variable patterns to retain.
          #
          # We need the KUBERNETES_ environment variables for the native Kubernetes support of Patroni to work.
          #
          # NB: Patroni will remove all PATRONI_.* environment variables before starting PostgreSQL

          # We store the current environment, as initscripts, callbacks, archive_commands etc. may require
          # to have the environment available to them
          set -o posix
          export -p > "${HOME}/.pod_environment"
          export -p | grep PGBACKREST > "${HOME}/.pgbackrest_environment"

          for UNKNOWNVAR in $(env | awk -F '=' '!/^(PATRONI_.*|HOME|PGDATA|PGHOST|LC_.*|LANG|PATH|KUBERNETES_SERVICE_.*)=/ {print $1}')
          do
              unset "${UNKNOWNVAR}"
          done

          echo "*:*:*:postgres:${PATRONI_SUPERUSER_PASSWORD}" >> ${HOME}/.pgpass
          chmod 0600 ${HOME}/.pgpass

          export PATRONI_POSTGRESQL_PGPASS="${HOME}/.pgpass.patroni"

          exec patroni /etc/timescaledb/patroni.yaml
        env:
        - name: PATRONI_admin_OPTIONS
          value: createrole,createdb
        - name: PATRONI_admin_PASSWORD
          valueFrom:
            secretKeyRef:
              key: admin
              name: timescaledb-single-chart-passwords
        - name: PATRONI_postgres_PASSWORD
          valueFrom:
            secretKeyRef:
              key: postgres
              name: timescaledb-single-chart-passwords
        - name: PATRONI_standby_PASSWORD
          valueFrom:
            secretKeyRef:
              key: standby
              name: timescaledb-single-chart-passwords
        - name: PATRONI_REPLICATION_PASSWORD
          valueFrom:
            secretKeyRef:
              key: standby
              name: timescaledb-single-chart-passwords
        - name: PATRONI_SUPERUSER_PASSWORD
          valueFrom:
            secretKeyRef:
              key: postgres
              name: timescaledb-single-chart-passwords
        - name: PATRONI_REPLICATION_USERNAME
          value: standby
        - name: PATRONI_KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PATRONI_POSTGRESQL_CONNECT_ADDRESS
          value: $(PATRONI_KUBERNETES_POD_IP):5432
        - name: PATRONI_RESTAPI_CONNECT_ADDRESS
          value: $(PATRONI_KUBERNETES_POD_IP):8008
        - name: PATRONI_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PATRONI_POSTGRESQL_DATA_DIR
          value: /var/lib/postgresql/data
        - name: PATRONI_KUBERNETES_NAMESPACE
          value: harness
        - name: PATRONI_KUBERNETES_LABELS
          value: '{app: timescaledb-single-chart, cluster-name: timescaledb-single-chart,
            release: timescaledb-single-chart}'
        - name: PATRONI_SCOPE
          value: timescaledb-single-chart
        - name: PGBACKREST_CONFIG
          value: /etc/pgbackrest/pgbackrest.conf
        - name: PGDATA
          value: $(PATRONI_POSTGRESQL_DATA_DIR)
        - name: PGHOST
          value: /var/run/postgresql
        image: timescaledev/timescaledb-ha:pg11.11-ts2.1.0-p1
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - psql
              - -X
              - --file
              - /etc/timescaledb/scripts/lifecycle_preStop.psql
        name: timescaledb
        ports:
        - containerPort: 8008
        - containerPort: 5432
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -h
            - /var/run/postgresql
          failureThreshold: 6
          initialDelaySeconds: 5
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: "1"
            memory: 2048Mi
          requests:
            cpu: "1"
            memory: 2048Mi
        securityContext:
          runAsUser: 1000
        volumeMounts:
        - mountPath: /var/lib/postgresql
          name: storage-volume
          subPath: ""
        - mountPath: /etc/timescaledb/patroni.yaml
          name: patroni-config
          readOnly: true
          subPath: patroni.yaml
        - mountPath: /etc/timescaledb/scripts
          name: timescaledb-scripts
          readOnly: true
        - mountPath: /etc/certificate
          name: certificate
          readOnly: true
        - mountPath: /var/run/postgresql
          name: socket-directory
        - mountPath: /etc/timescaledb/callbacks
          name: patroni-callbacks
          readOnly: true
      initContainers: null
      securityContext:
        fsGroup: 1000
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 600
      volumes:
      - emptyDir: {}
        name: socket-directory
      - configMap:
          name: timescaledb-single-chart-patroni
        name: patroni-config
      - configMap:
          defaultMode: 488
          name: timescaledb-single-chart-scripts
        name: timescaledb-scripts
      - configMap:
          defaultMode: 488
          name: timescaledb-init
        name: patroni-callbacks
      - name: certificate
        secret:
          defaultMode: 416
          secretName: timescaledb-single-chart-certificate
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      annotations: null
      labels:
        app: timescaledb-single-chart
        cluster-name: timescaledb-single-chart
        heritage: Tiller
        purpose: data-directory
        release: timescaledb-single-chart
      name: storage-volume
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 120Gi
      storageClassName: standard
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
    kubernetes.io/ingress.class: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: delegate-proxy
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: delegate-proxy
          servicePort: 80
        path: /storage
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
    kubernetes.io/ingress.class: harness
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-gateway
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: harness-gateway
          servicePort: 80
        path: /gateway(/|$)(.*)
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
    kubernetes.io/ingress.class: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-manager
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: harness-manager
          servicePort: 9090
        path: /api
      - backend:
          serviceName: harness-manager
          servicePort: 9090
        path: /stream
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
    kubernetes.io/ingress.class: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: harness-ui
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: harness-ui
          servicePort: 80
        path: /
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kots.io/app-slug: harness
    kubernetes.io/ingress.class: harness
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: manager-grpc
spec:
  rules:
  - host: manager-grpc-harness-url
    http:
      paths:
      - backend:
          serviceName: harness-manager
          servicePort: manager-grpc
        path: /
  tls:
  - hosts:
    - manager-grpc-harness-url
    secretName: harness-cert
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
    kubernetes.io/ingress.class: harness
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: ng-auth-ui
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: ng-auth-ui
          servicePort: 80
        path: /auth(/|$)(.*)
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kots.io/app-slug: harness
    kots.io/placeholder: "true"
    kubernetes.io/ingress.class: harness
  labels:
    kots.io/app-slug: harness
    kots.io/backup: velero
  name: verification-svc
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: verification-svc
          servicePort: 7070
        path: /verification
